{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 264,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.3088803088803089,
      "grad_norm": 334.0333557128906,
      "learning_rate": 9.727272727272728e-06,
      "loss": 78.5867,
      "step": 10
    },
    {
      "epoch": 0.6177606177606177,
      "grad_norm": 322.85345458984375,
      "learning_rate": 9.424242424242425e-06,
      "loss": 72.3084,
      "step": 20
    },
    {
      "epoch": 0.9266409266409267,
      "grad_norm": 312.45513916015625,
      "learning_rate": 9.121212121212122e-06,
      "loss": 64.1678,
      "step": 30
    },
    {
      "epoch": 1.2162162162162162,
      "grad_norm": 276.5628662109375,
      "learning_rate": 8.818181818181819e-06,
      "loss": 50.9883,
      "step": 40
    },
    {
      "epoch": 1.525096525096525,
      "grad_norm": 251.21766662597656,
      "learning_rate": 8.515151515151517e-06,
      "loss": 46.1662,
      "step": 50
    },
    {
      "epoch": 1.833976833976834,
      "grad_norm": 214.87425231933594,
      "learning_rate": 8.212121212121212e-06,
      "loss": 35.6866,
      "step": 60
    },
    {
      "epoch": 2.1235521235521237,
      "grad_norm": 196.16307067871094,
      "learning_rate": 7.909090909090909e-06,
      "loss": 25.5702,
      "step": 70
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 159.97486877441406,
      "learning_rate": 7.606060606060606e-06,
      "loss": 19.7364,
      "step": 80
    },
    {
      "epoch": 2.741312741312741,
      "grad_norm": 123.62477111816406,
      "learning_rate": 7.303030303030304e-06,
      "loss": 14.6775,
      "step": 90
    },
    {
      "epoch": 3.030888030888031,
      "grad_norm": 93.29579162597656,
      "learning_rate": 7e-06,
      "loss": 9.3542,
      "step": 100
    },
    {
      "epoch": 3.33976833976834,
      "grad_norm": 78.67496490478516,
      "learning_rate": 6.6969696969696975e-06,
      "loss": 7.0151,
      "step": 110
    },
    {
      "epoch": 3.6486486486486487,
      "grad_norm": 55.880943298339844,
      "learning_rate": 6.393939393939394e-06,
      "loss": 4.9823,
      "step": 120
    },
    {
      "epoch": 3.9575289575289574,
      "grad_norm": 32.87015151977539,
      "learning_rate": 6.090909090909092e-06,
      "loss": 3.3621,
      "step": 130
    },
    {
      "epoch": 4.2471042471042475,
      "grad_norm": 21.495664596557617,
      "learning_rate": 5.787878787878788e-06,
      "loss": 2.3531,
      "step": 140
    },
    {
      "epoch": 4.555984555984556,
      "grad_norm": 18.035717010498047,
      "learning_rate": 5.484848484848485e-06,
      "loss": 2.0299,
      "step": 150
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 16.33654022216797,
      "learning_rate": 5.181818181818182e-06,
      "loss": 1.8205,
      "step": 160
    },
    {
      "epoch": 5.154440154440154,
      "grad_norm": 16.872995376586914,
      "learning_rate": 4.878787878787879e-06,
      "loss": 1.4499,
      "step": 170
    },
    {
      "epoch": 5.463320463320463,
      "grad_norm": 17.276865005493164,
      "learning_rate": 4.575757575757576e-06,
      "loss": 1.5654,
      "step": 180
    },
    {
      "epoch": 5.772200772200772,
      "grad_norm": 16.844175338745117,
      "learning_rate": 4.272727272727273e-06,
      "loss": 1.5312,
      "step": 190
    },
    {
      "epoch": 6.061776061776062,
      "grad_norm": 13.693013191223145,
      "learning_rate": 3.96969696969697e-06,
      "loss": 1.2768,
      "step": 200
    },
    {
      "epoch": 6.370656370656371,
      "grad_norm": 12.114012718200684,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 1.327,
      "step": 210
    },
    {
      "epoch": 6.67953667953668,
      "grad_norm": 10.651598930358887,
      "learning_rate": 3.3636363636363637e-06,
      "loss": 1.3662,
      "step": 220
    },
    {
      "epoch": 6.988416988416988,
      "grad_norm": 9.233905792236328,
      "learning_rate": 3.0606060606060605e-06,
      "loss": 1.1134,
      "step": 230
    },
    {
      "epoch": 7.277992277992278,
      "grad_norm": 9.723097801208496,
      "learning_rate": 2.7575757575757576e-06,
      "loss": 1.0709,
      "step": 240
    },
    {
      "epoch": 7.586872586872587,
      "grad_norm": 7.558409690856934,
      "learning_rate": 2.454545454545455e-06,
      "loss": 1.1605,
      "step": 250
    },
    {
      "epoch": 7.895752895752896,
      "grad_norm": 7.6143879890441895,
      "learning_rate": 2.1515151515151515e-06,
      "loss": 1.1641,
      "step": 260
    }
  ],
  "logging_steps": 10,
  "max_steps": 330,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.24018048551977e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
